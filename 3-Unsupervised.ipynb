{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 3 : Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Notebook prepared by [Chloé-Agathe Azencott](http://cazencott.info), modified by [Victor Laigle](https://eaglev-sci.github.io/).\n",
    "\n",
    "In this notebook we will explore several techniques for dimension reduction and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load numpy, pandas and matplotlib (with aliases np, pd and plt respectively)\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 12}) # set the global font size for plots (in pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Principal Components Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "In this section, we are going to perform a principal component analysis on a dataset describing the scores obtained by the best athletes who participated in a decathlon event in 2004, at the Athens Olympic Games or at the Talence Decastar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "The data are contained in the `decathlon.txt` file.\n",
    "\n",
    "The file contains 42 rows and 13 columns.\n",
    "\n",
    "The first line is a header that describes the contents of the columns.\n",
    "\n",
    "The following lines describe the 41 athletes.\n",
    "\n",
    "The first 10 columns contain the scores obtained in the different events of the Decathlon.\n",
    "The 11th column contains the ranking.\n",
    "The 12th column contains the number of points obtained.\n",
    "The 13th column contains a categorical variable that specifies the competition concerned (Olympics or Decastar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "We will start by downloading the data from the github repository to your current working directory, and load the data into a `pandas` `dataframe`. \n",
    "\n",
    "If you already downloaded the data, you can use the alternative suggested in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems-en/main/data/decathlon.txt\n",
    "\n",
    "my_data = pd.read_csv('decathlon.txt', sep=\"\\t\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "__Alternatively:__ If you're not on Colab and have already downloaded the file to a `data` folder, uncomment and run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data = pd.read_csv('data/decathlon.txt', sep=\"\\t\")  # Read the data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "A __scatter matrix__ is a visualisation, in k x k panels, of the pairwise relations between k variables:\n",
    "* on the diagonal, the histogram of each variable\n",
    "* off-diagonal, the scatter plots between two variables (non standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Your job here is to display the visualization with the [`scatter_matrix`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html) function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "### START OF YOUR CODE\n",
    "\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "You can also limit the visualization to a few variables for clearer observations. Display the scatter_matrix for the 3 or 4 variables which seem most correlated for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Alternatively, the `seaborn` library  la librairie `seaborn` enables more elaborated visualizations than `matplotlib`. You can explore, for instance, the capabilities of [`jointplot`](https://seaborn.pydata.org/generated/seaborn.jointplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "sns.jointplot(x='Shot.put', y='400m', data = my_data, \n",
    "              height=6, space=0, color='b')\n",
    "\n",
    "sns.jointplot(x='Rank', y='Points', data = my_data, \n",
    "              kind='reg', height=6, space=0, color='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We will now perform a principal components analysis of the scores in the 10 events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Let's start by extracting the predictive variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(my_data.drop(columns=['Points', 'Rank', 'Competition']))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "After visualizing the data, we can notice different data scales and distributions depending on the variables. We therefore reapply the procedure seen in previous labs to standardize our data: we need a `StandardScaler` object contained in the `preprocessing` module of `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "# Import the module\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the StandardScaler object\n",
    "std_scaler = ...\n",
    "\n",
    "# Fit the object to the data\n",
    "...\n",
    "\n",
    "# Transform the data\n",
    "X_scaled = ...\n",
    "\n",
    "### END OF YOUR CODE\n",
    "\n",
    "print(X_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Computation of the principal components\n",
    "\n",
    "`scikit-learn`'s algorithms for matrix factorization are included in the `decomposition` module. For PCA, refer to:\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Note: We have few variables here so we can afford to calculate all the PCs.\n",
    "\n",
    "As we've seen in previous labs, most algorithms implemented in `scikit-learn` work as follows:\n",
    "* we instantiate an object, corresponding to a type of algorithm/model, with its hyperparameters (here the number of components)\n",
    "* we use the `fit` method to pass the data to this algorithm\n",
    "* the learned parameters are now accessible as attributes of this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of a PCA object with 10 principal components\n",
    "\n",
    "### START OF YOUR CODE\n",
    "pca = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now pass the standardized data to this object\n",
    "# This is where the computations are performed\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Proportion of variance explained by the PCs\n",
    "\n",
    "We will now plot the proportion of variance explained by the different components. This is accessible in the attribute `explained_variance_ratio_` of our `pca` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, 11), pca.explained_variance_ratio_, marker='o')\n",
    "\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Proportion of variance explained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "We can also display the *cumulative* proportion of variance explained, with the function [`cumsum`](https://numpy.org/doc/2.1/reference/generated/numpy.cumsum.html) from `numpy` (imported above under the alias `np`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Create a similar graph to the one just above, displaying the cumulative proportion of variance explained as a function of the number of principal components considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "...\n",
    "\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "__Questions:__ \n",
    "* What is the proportion of variance explained by the first two components ?\n",
    "* How many components would we need to use to explain 80% of the data's variance ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Data projection on the first two principal components\n",
    "\n",
    "We will now use only the first two principal components.\n",
    "\n",
    "Let's start by computing the new data representation, i.e. their projection on these two PCs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_projected = pca.transform(X_scaled)\n",
    "print(X_projected.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "We can display a scatter plot representing the data along these two PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "We can now color each point of the above scatter plot according to the ranking of the athlete it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "plt.scatter(X_projected[:, 0], X_projected[:, 1], c=my_data['Rank'])\n",
    "\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.colorbar(label='Rank')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "__Question:__ What can we conclude about the interpretation of PC1 ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Interpretation of the first two principal components\n",
    "\n",
    "Each principal component is a linear combination of the variables describing the data. The weights of this linear combination are accessible in `pca.components_`.\n",
    "\n",
    "We can now visualize, not the individuals as above, but the 10 variables in the space of the 2 PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = pca.components_\n",
    "print(pcs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "plt.scatter(pcs[0], pcs[1])\n",
    "for (x_coordinate, y_coordinate, feature_name) in zip(pcs[0], pcs[1], my_data.columns[:10]):\n",
    "    plt.text(x_coordinate + 0.01, y_coordinate + 0.01, feature_name)                          \n",
    "    \n",
    "plt.xlabel(\"Contribution to PC1\")\n",
    "plt.ylabel(\"Contribution to PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "__Questions:__ \n",
    "* Which variables have contributions very similar to the two principal components ?\n",
    "* What can we deduce from their similarity ?\n",
    "* How to interpret the sign of the variables' contributions to the _first_ principal component ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## 2. « Olivetti » data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "We will now use dimension reduction to represent, in two dimensions, a dataset of human faces. This is a classic dataset, containing 400 pictures of 64 by 64 pixels. These are pictures of the face of 40 different people, 10 pictures per person, labeled by a class number between 0 and 39 to identify the person.  \n",
    "\n",
    "We can load this dataset directly thanks to `scikit-learn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "__If you do not manage to download the data:__\n",
    "* Go to : https://github.com/CroncLee/PCA-face-recognition/blob/master/olivetti_py3.pkz\n",
    "* Download the file (there is a Download button)\n",
    "* Use the command\n",
    "```\n",
    "    data = datasets.fetch_olivetti_faces(data_home=\"<PATH TO DATA>\")\n",
    "```\n",
    "Replace \"PATH TO DATA\" by the path of the folder where you saved the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains %d classes\" % len(np.unique(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Each image is represented by one value for each pixel (grey scale).\n",
    "\n",
    "We may visualize these images, under the condition that we reorganize the values (= a vector of length 4096) in 64x64 matrices. For example, for image with index 77:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X[77, :].reshape((64, 64)), interpolation='nearest', cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PCA\n",
    "\n",
    "Let's start with a principal components analysis as in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "X_transformed_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Each picture is now represented, not by 4096 variables, but by 2 variables. We can visualize them on a scatter plot and color them according to their class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed_pca[:, 0], X_transformed_pca[:, 1], c=y)\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "__Question:__ Do pictures of the same face (= same class) have close or far apart representations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "We can visualize the contribution of each pixel to the first principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.components_[0, :].reshape((64,64)), interpolation='nearest', cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "And the contribution of each pixel to the second principal component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pca.components_[1, :].reshape((64,64)), interpolation='nearest', cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "__Question:__ Which interpretation can we draw from these two images representing the contributions of each pixel to the first two principal components ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### tSNE\n",
    "\n",
    "Let's try another dimension reduction method to better separate our classes.\n",
    "\n",
    "We will use the same approach as for PCA, but with the tSNE algorithm, thanks to the [TSNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) class of the `manifold` module.\n",
    "\n",
    "Now it's your turn to:\n",
    "- create the `tsne` object from the class cited above. Here we want to reduce our dataset to two dimensions in order to visualize it.\n",
    "- assign the information of our dataset to this object\n",
    "- transform our data to obtain new coordinates in two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "tsne = ...\n",
    "X_transformed = ...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Let's display the result of the tSNE dimension reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y)\n",
    "plt.xlabel(\"Première composante tSNE\")\n",
    "plt.ylabel(\"Deuxième composante tSNE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "#### Influence of the perplexity parameter\n",
    "\n",
    "The main hyperparameter influencing the representation obtained by the tSNE algorithm is the \"perplexity\" parameter. This represents the number of neighbours for which distances are preserved. It therefore influences the preservation of the local structure (low perplexity) or global structure (high perplexity). The representation obtained can vary greatly depending on this parameter.\n",
    "\n",
    "Test different values of the perplexity parameter and display the corresponding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "tsne_low_perp = ...\n",
    "X_transformed_low_perp = ...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed_low_perp[:, 0], X_transformed_low_perp[:, 1], c=y)\n",
    "plt.xlabel(\"First tSNE component\")\n",
    "plt.ylabel(\"Second tSNE component\")\n",
    "plt.title(\"tSNE (low perplexity)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "tsne_high_perp = ...\n",
    "X_transformed_high_perp = ...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed_high_perp[:, 0], X_transformed_high_perp[:, 1], c=y)\n",
    "plt.xlabel(\"First tSNE component\")\n",
    "plt.ylabel(\"Second tSNE component\")\n",
    "plt.title(\"tSNE (high perplexity)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "We will now cover another set of unsupervised methods, used to group observations according to their similarities.\n",
    "\n",
    "We'll start by generating three two-dimensional datasets:\n",
    "- 4 separated blobs from normal distributions\n",
    "- 2 nested semi-circles (or \"semi-moons\")\n",
    "- 2 concentric circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de points\n",
    "n_samples = 1000\n",
    "\n",
    "four_blobs, four_blobs_labels = datasets.make_blobs(n_samples=n_samples, centers=4, n_features=2, random_state=170)\n",
    "\n",
    "moons, moons_labels = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=170)\n",
    "\n",
    "circles, circles_labels = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05, random_state=170)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "First, let's visualize these data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "ax[0].scatter(four_blobs[:, 0], four_blobs[:, 1], c=four_blobs_labels, s=20, alpha=0.7, cmap='viridis')\n",
    "ax[1].scatter(moons[:, 0], moons[:, 1], c=moons_labels, s=20, alpha=0.7, cmap='viridis')\n",
    "ax[2].scatter(circles[:, 0], circles[:, 1], c=circles_labels, s=20, alpha=0.7, cmap='viridis')\n",
    "\n",
    "ax[0].set_title('4 blobs')\n",
    "ax[1].set_title('Moons')\n",
    "ax[2].set_title('Circles');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "Now, let's assume that we do **not** have access to the labels. Which clustering algorithms allow us to recover the clusters corresponding respectively to the four blobs, two moons and two circles ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### K-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "The objective of the k-means algorithm is to find $K$ clusters (and their centroid $\\mu_k$) so as to **minimize the intra-cluster variance**:\n",
    "\n",
    "\\begin{align}\n",
    "V = \\sum_{k = 1}^{K} \\sum_{x \\in C_k} \\frac{1}{|C_k|} (\\|x - \\mu_k\\|^2)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "**Manual implementation**\n",
    "\n",
    "We'll start by implementing the algorithm \"by hand\", step by step, to fully understand and visualize what the algorithm does. We will then see how to use directly the K-means implementation in the `sklearn` library. The different steps of the algorithm are:\n",
    "1. Select the number `k` of clusters (hyperparameter)\n",
    "2. Initialize the `k` centroids at random among our data points\n",
    "3. Compute the distances from all data points to these centroids\n",
    "4. Assign each point to the cluster of the closest centroid\n",
    "5. Compute the position of the new centroids\n",
    "6. Repeat steps 3 to 5 until convergence, i.e. until the centroids no longer change from one iteration to the next\n",
    "\n",
    "We will implement this algorithm on the 4 blobs dataset, starting with the choice of `k` and with the random selection of `k` points from our dataset that will constitute the initial centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23)  # set seed. Important for k-means step by step visualisation. With other seeds, it's not as clear how the algorithm works.\n",
    "\n",
    "k = 4\n",
    "random_indices = np.random.choice(len(four_blobs), k, replace=False)\n",
    "centroids_step0 = four_blobs[random_indices]\n",
    "\n",
    "for i, centroid in enumerate(centroids_step0):\n",
    "    print(f\"Centroid {i} : x = {centroid[0]},\\ty = {centroid[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Let's look at the data with these initial centroids (red crosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "\n",
    "ax.scatter(four_blobs[:, 0], four_blobs[:, 1], c='grey', s=20, alpha=0.7)\n",
    "ax.scatter(centroids_step0[:, 0], centroids_step0[:, 1], c='red', marker='x')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "We can now compute the distances between each data point and these centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance between data point and centroid\n",
    "def compute_distance(data_point, centroid):\n",
    "    dist = np.sqrt(np.sum((data_point - centroid)**2))\n",
    "    return dist\n",
    "\n",
    "def compute_all_distances(dataset, centroids):\n",
    "    # Initialize distances array\n",
    "    distances = np.zeros((k, n_samples))  # k and n_samples defined above\n",
    "    \n",
    "    # Calculate distance from each point to each centroid\n",
    "    for i in range(k):\n",
    "        for j in range(n_samples):\n",
    "            distances[i, j] = compute_distance(dataset[j], centroids[i])    \n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = compute_all_distances(four_blobs, centroids_step0)\n",
    "\n",
    "# Example of distances for the first 5 points to the k centroids\n",
    "print(\"Distances :\")\n",
    "print(\"\\t\\t\", \" \\t\\t\".join([f\"Point {i+1}\" for i in range(5)]))\n",
    "for i in range(k):\n",
    "    print(f\"Centroid {i}\\t\", \"\\t\".join(distances[i, :5].astype(str).tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "We now assign to each point the cluster corresponding to the closest centroid. We use the `argmin` function from `numpy` for that. In a way it can be seen as intermediate predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_cluster(distances):\n",
    "    assignments = np.argmin(distances, axis=0)\n",
    "    return assignments\n",
    "\n",
    "intermediate_labels = assign_cluster(distances)\n",
    "print(intermediate_labels[:5])  # examples of intermediate labels assigned to the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "We can verify that the intermediate labels assigned indeed correspond to the closest centroid by comparing with the distances calculated above (see previous cell).\n",
    "\n",
    "Let's visualize these intermediate clusters and the initial centroids on a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_kmeans(dataset, labels, centroids, ax):\n",
    "\n",
    "    ax.scatter(dataset[:, 0], dataset[:, 1], c=labels, s=20, alpha=0.7, cmap='viridis')\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "visualise_kmeans(four_blobs, intermediate_labels, centroids_step0, ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "We now need to compute the position of the new centroids, which we will plot on a new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_new_centroids(dataset, labels):\n",
    "    centroids = np.zeros((k, dataset.shape[1]))\n",
    "\n",
    "    for i in range(k):\n",
    "        centroids[i] = dataset[labels == i].mean(axis=0)\n",
    "    \n",
    "    return centroids\n",
    "\n",
    "centroids_step1 = compute_new_centroids(four_blobs, intermediate_labels)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "visualise_kmeans(four_blobs, intermediate_labels, centroids_step0, axes[0])\n",
    "visualise_kmeans(four_blobs, intermediate_labels, centroids_step1, axes[1])\n",
    "\n",
    "axes[0].set_title(\"Initial centroids\")\n",
    "axes[1].set_title(\"Centroids after one iteration\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "We now repeat the different steps: \n",
    "- computation of distances to centroids,\n",
    "- assignment of clusters to data points,\n",
    "- computation of new centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 5, figsize=(24, 4))\n",
    "n_iter = 10  # Number of iterations of the algorithm\n",
    "\n",
    "current_fig = 0\n",
    "\n",
    "centroids = centroids_step1\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    distances = compute_all_distances(four_blobs, centroids)\n",
    "    intermediate_labels = assign_cluster(distances)\n",
    "\n",
    "    # We display the visualization every 2 iterations\n",
    "    if (i+1) % 2 == 0:\n",
    "        visualise_kmeans(four_blobs, intermediate_labels, centroids, axes[current_fig])\n",
    "        axes[current_fig].set_title(f\"Iteration {i+1}\")\n",
    "        current_fig += 1\n",
    "        \n",
    "    centroids = compute_new_centroids(four_blobs, intermediate_labels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "We clearly see here how, through successive iterations, the algorithm is able to correctly identify our clusters, gradually moving the centroids and readjusting the clusters' membership of our data points.\n",
    "\n",
    "**Question**: In which case(s) can the algorithm yield bad results ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "**Implementation with `sklearn`**\n",
    "\n",
    "Let's come back to the 3 datasets created earlier (4 blobs, semi-moons and concentric circles), and apply the  to them the `sklearn` implementation of the K-means algorithm, which is an optimized version of the steps we have just seen.\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START OF YOUR CODE\n",
    "\n",
    "# Instantiation of the three k-means models with the\n",
    "# theoretical number of clusters (resp. 4, 2 and 2):\n",
    "kmeans_four_blobs = ...\n",
    "kmeans_moons = ...\n",
    "kmeans_circles = ...\n",
    "\n",
    "# Application to the data\n",
    "...\n",
    "...\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "The `.labels_` attribute contains, for each observation, the number of the cluster to which this observation has been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_four_blobs.labels_[0:10]  # Example of predicted labels for the first 10 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "Let's visualize what the final clustering looks like for our three datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering visualization\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 3))\n",
    "\n",
    "ax[0].scatter(four_blobs[:, 0], four_blobs[:, 1], c=kmeans_four_blobs.labels_, cmap='viridis')\n",
    "ax[1].scatter(moons[:, 0], moons[:, 1], c=kmeans_moons.labels_, cmap='viridis')\n",
    "ax[2].scatter(circles[:, 0], circles[:, 1], c=kmeans_circles.labels_, cmap='viridis')\n",
    "\n",
    "ax[0].set_title('4 blobs (k=4)')\n",
    "ax[1].set_title('Moons (k=2)')\n",
    "ax[2].set_title('Circles (k=2)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "- Is it the expected clustering ? \n",
    "- In which case(s) does the k-means algorithm work correctly ?\n",
    "- Why doesn't it work in the other case(s) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "#### Finding $K$ with the silhouette coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "It is often the case that the exact number of clusters, $K$, is not known in advance. We can still apply the k-means algorithm and measure the clustering performance to find the best parameter $K$. One of the metrics used is the **silhouette coefficient**.\n",
    "\n",
    "The silhouette coefficient (or score) enables to **compare the average intra- and inter-cluster distances**:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{score} = \\frac{b - a}{\\max(a, b)}\n",
    "\\end{align}\n",
    "\n",
    "with (for each sample):\n",
    "- $a$ the average intra-cluster distance\n",
    "- $b$ the average nearest-cluster distance\n",
    "\n",
    "The score is computed for each observation (with a value between -1 (worst) and 1 (best)), then the average score enables to assess the clustering on the whole point cloud at once.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"4 blobs: Silhouette coefficient for k-means (k=4) : %.2f\" % \n",
    "      metrics.silhouette_score(four_blobs, kmeans_four_blobs.labels_))\n",
    "print(f\"Moons: Silhouette coefficient for  k-means (k=2) : %.2f\" % \n",
    "      metrics.silhouette_score(moons, kmeans_moons.labels_))\n",
    "print(f\"Circles: Silhouette coefficient for  k-means (k=2) : %.2f\" % \n",
    "      metrics.silhouette_score(circles, kmeans_circles.labels_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "Let's try to evaluate the performance of the clustering as a function of the number of clusters $K$, by varying it in the range [2, .., 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 9)\n",
    "names = ['4 blobs', 'Moons', 'Circles']\n",
    "fig, ax = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "for i, dataset in enumerate([four_blobs, moons, circles]):\n",
    "    silhouettes = []\n",
    "    \n",
    "    for kval in k_values:\n",
    "\n",
    "        ### START OF YOUR CODE\n",
    "\n",
    "        # Initialize a KMeans model with the number of cluster tested:\n",
    "        kmeans_k = ...\n",
    "        \n",
    "        # Train the model on the data\n",
    "        ...\n",
    "        \n",
    "        # Append the silhouette score obtained to the list\n",
    "        ...\n",
    "        \n",
    "        ### END OF YOUR CODE\n",
    "    \n",
    "    # Visualization of the silhouette score\n",
    "    ax[0,i].plot(k_values, silhouettes)\n",
    "    ax[0,i].set_xlabel(\"K\")\n",
    "    ax[0,i].set_ylabel(\"Silhouette score\")\n",
    "    ax[0,i].set_title(names[i])\n",
    "    \n",
    "    print(\"Dataset:\", names[i])\n",
    "    best_silhouette = np.max(silhouettes)\n",
    "    print(\"Optimal silhouette coefficient: %.2f\" % best_silhouette)\n",
    "    best_K = k_values[silhouettes.index(best_silhouette)]\n",
    "    print(\"Corresponding number of cluster K: %.0f\" % best_K)\n",
    "    \n",
    "    # Final clustering with the best K\n",
    "    kmeans_k = cluster.KMeans(n_clusters=best_K)\n",
    "    kmeans_k.fit(dataset)\n",
    "    ax[1,i].scatter(dataset[:, 0], dataset[:, 1], c=kmeans_k.labels_, cmap='viridis')\n",
    "    ax[1,i].set_xlabel('x1')\n",
    "    ax[1,i].set_ylabel('x2')\n",
    "    ax[1,i].set_title('Clustering with ' + str(best_K) + ' clusters')\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "**Conclusions:** \n",
    "- The k-means algorithm provides a satisfactory clustering for the dataset with four well-separated blobs, even without knowing the ideal number of clusters in advance, in which case the silhouette coefficient allows to find this ideal number.\n",
    "- However, despite optimizing the silhouette score, this algorithm does not provide good results for the other datasets, whether for the two nested moons or the two concentric circles.\n",
    "\n",
    "We will therefore now try another clustering algorithm and test its performance to compare it to k-means. We will limit ourselves to the two datasets for which the k-means algorithm does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DBSCAN (Density-based clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "The DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm works in two steps:\n",
    "- All sufficiently close observations are connected to each other.\n",
    "- Observations with a minimum number of connected neighbors are considered *core samples*, from which the clusters are expanded. **All observations sufficiently close to a *core sample* belong to the same cluster as this one**.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "The DBSCAN algorithm takes two hyperparameters as input:\n",
    "- `eps`: the *size of the neighborhood*, in other words, the distance between two data points below which one point is considered inside the neighborhood of the other.\n",
    "- `min_samples`: the minimum number of neighbors for a data point to be considered a *core sample*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "# Initialization of the two DBSCAN models\n",
    "# with hyperparameters eps=0.2, min_samples=2:\n",
    "dbscan_moons = ...\n",
    "dbscan_circles = ...\n",
    "\n",
    "# Fit to the data\n",
    "...\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "Again, the `.labels_` attribute contains, for each observation, the number of the cluster to which this observation has been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of labels for the moons dataset:\", len(np.unique(dbscan_moons.labels_)))\n",
    "print(\"Number of labels for the circles dataset:\", len(np.unique(dbscan_circles.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "Let's visualize the clusters obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax[0].scatter(moons[:, 0], moons[:, 1], c=dbscan_moons.labels_, cmap='viridis')\n",
    "ax[0].set_title(\"Clustering DBSCAN (eps=0.2)\")\n",
    "\n",
    "ax[1].scatter(circles[:, 0], circles[:, 1], c=dbscan_circles.labels_, cmap='viridis')\n",
    "ax[1].set_title(\"Clustering DBSCAN (eps=0.2)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "We can see here that the DBSCAN algorithm is able to identify the two respective clusters in the two datasets that were problematic for the k-means algorithm.\n",
    "\n",
    "We can also note that we did not need to provide a number of clusters beforehand for the algorithm to correctly identify the right number of clusters. However, the algorithm is sensitive to the two hyperparameters mentioned above, which we will now evaluate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "#### Role of the neighborhood size parameter (`eps`)\n",
    "\n",
    "We will assess the influence of the `eps` parameter on the concentric circles dataset (`circles`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "# Choose low and high values for eps, for example 0.05 and 2.0\n",
    "eps_low = ...\n",
    "eps_high = ...\n",
    "\n",
    "# Instantiation of a DBSCAN clustering object with low eps and another one with high eps\n",
    "dbscan_low = ...\n",
    "dbscan_high = ...\n",
    "\n",
    "# Fit to the data\n",
    "...\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "**Question:** What is the number of clusters obtained in our two models ? (Use the `.labels` attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START OF YOUR CODE\n",
    "\n",
    "...\n",
    "...\n",
    "\n",
    "### END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "outliers = np.where(dbscan_low.labels_ == -1)[0]\n",
    "plt.scatter(circles[outliers, 0], circles[outliers, 1], marker='*', color='red')\n",
    "\n",
    "non_outliers = np.where(dbscan_low.labels_ != -1)[0]\n",
    "plt.scatter(circles[non_outliers, 0], circles[non_outliers, 1], c=dbscan_low.labels_[non_outliers])\n",
    "plt.title(f\"Clustering DBSCAN (eps={eps_low})\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c=dbscan_high.labels_)\n",
    "plt.title(f\"Clustering DBSCAN (eps={eps_high})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Trouver eps avec le coefficient de silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette coefficient for DBSCAN (eps=0.2) : %.2f\" % \n",
    "      metrics.silhouette_score(circles, dbscan_circles.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_values = np.logspace(-3, 1, 40)\n",
    "silhouettes = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan_eps = cluster.DBSCAN(eps=eps, min_samples=2)\n",
    "    dbscan_eps.fit(circles)\n",
    "    if len(np.unique(dbscan_eps.labels_)) > 1:  # Needed to compute the silhouette coeff\n",
    "        silhouettes.append(metrics.silhouette_score(circles, dbscan_eps.labels_))\n",
    "    else:\n",
    "        silhouettes.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps_values, silhouettes)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"eps (log scale)\")\n",
    "plt.ylabel(\"silhouette\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = np.max(silhouettes)\n",
    "print(f\"Optimal silhouette coefficient: {best_silhouette:.2f}\")\n",
    "print(f\"Corresponding eps: {eps_values[silhouettes.index(best_silhouette)]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "Let's see what the clustering looks like using the `eps` parameter yielding the best silhouette coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_eps = eps_values[silhouettes.index(best_silhouette)]\n",
    "\n",
    "dbscan_best = cluster.DBSCAN(eps=best_eps, min_samples=2)\n",
    "dbscan_best.fit(circles)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(circles[:, 0], circles[:, 1], c=dbscan_best.labels_)\n",
    "plt.title(f\"Clustering DBSCAN (eps={best_eps:.2f})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "**Question:** \n",
    "- What is the problem here ?\n",
    "- Is the silhouette coefficient adapted to our dataset ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "### Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "The adjusted Rand index enables to **compare a clustering's result with labels**. For each pair of observations, we look at whether they belong to the same cluster or not, in the predicted and the true clustering. The index takes values between 0 (random clustering) and 1 (perfect clustering).\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144",
   "metadata": {},
   "source": [
    "__Question:__ Why not use an evaluation metric from classification models here ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted Rand Index of K-means (K=2) : %.2f\" % metrics.adjusted_rand_score(circles_labels, kmeans_circles.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted Rand Index of DBSCAN (eps=0.2) : %.2f\" % metrics.adjusted_rand_score(circles_labels, dbscan_circles.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "## Bonus: Clustering on Penguins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "The goal here is to test several unsupervised clustering methods on a new dataset, and to compare them. Try the methods seen above such as [sklearn.cluster.KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) or [sklearn.cluster.DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). You can also try other methods such as Gaussian mixtures ([sklearn.mixture.GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)).\n",
    "\n",
    "Which methods do you think should work better? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "The dataset we suggest you use here concerns different species of penguins and some physical characteristics. The 3 species are: Adelie penguins, Gentoo penguins and Chinstrap penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems-en/main/data/penguins_data.csv\n",
    "\n",
    "palmerpenguins = pd.read_csv(\"penguins_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "__Alternatively:__ If you already downloaded the data, uncomment the following line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# palmerpenguins = pd.read_csv(\"data/penguins_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "palmerpenguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "The first step is to filter some data to avoid missing values (NA or NaN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "palmerpenguins = palmerpenguins[palmerpenguins['bill_depth_mm'].notna()]\n",
    "palmerpenguins = palmerpenguins.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156",
   "metadata": {},
   "source": [
    "We will focus only on two characteristics of our penguins: beak length and body mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_X = np.array(palmerpenguins[[\"bill_length_mm\", \"body_mass_g\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "Our variables have very different scales, as we can see in the data displayed above. Therefore, we need to standardize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization (centering and scaling)\n",
    "penguins_X = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_names, species_int = np.unique(palmerpenguins.species, return_inverse=True)\n",
    "penguins_labels = species_int\n",
    "species_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "Let's first visualize our data on a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels)\n",
    "plt.xlabel(\"bill_length_mm (scaled)\")\n",
    "plt.ylabel(\"body_mass_g (scaled)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "It's now up to you to test different clustering algorithms on these data, and evaluate their performance. Will you be able to obtain a perfect clustering ?\n",
    "\n",
    "In addition to the clustering algorithms used above, you can try Gaussian mixtures ([GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)) for example, or other methods from the `cluster` or `mixture` modules. For each, provide the silhouette coefficients and adjusted Rand indices you obtain, as well as a visualization of the resulting clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of a k-means model with k=3\n",
    "kmeans = ...\n",
    "\n",
    "# Fit to the data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels, marker='o')\n",
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=kmeans.labels_, marker='*')\n",
    "\n",
    "\n",
    "plt.xlabel(\"bill_length_mm (scaled)\")\n",
    "plt.ylabel(\"body_mass_g (scaled)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette coefficient for k-means (k=3) : %.2f\" % metrics.silhouette_score(penguins_X, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted Rand Index for K-means (K=3) : %.2f\" % metrics.adjusted_rand_score(penguins_labels, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_values = np.logspace(-3, 1, 40)\n",
    "silhouettes = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    dbscan_eps = ...\n",
    "    ...\n",
    "    if len(np.unique(dbscan_eps.labels_)) > 1: # needed to compute silhouette coeff\n",
    "        silhouettes.append(...)\n",
    "    else:\n",
    "        silhouettes.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps_values, silhouettes)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"eps (log scale)\")\n",
    "plt.ylabel(\"silhouette\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_silhouette = ...\n",
    "print(\"Optimal silhouette coefficient: %.2f\" % best_silhouette)\n",
    "best_eps = ...\n",
    "print(\"Corresponding eps: %.2f\" % best_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174",
   "metadata": {},
   "source": [
    "Retrieve the eps and min_samples parameters from the optimal DBSCAN model, create a new model with these and fit it to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_opt = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dbscan_opt.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted Rand Index for DBSCAN : %.2f\" % metrics.adjusted_rand_score(penguins_labels, dbscan_opt.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels, marker='o')\n",
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=dbscan_opt.labels_, marker='*')\n",
    "\n",
    "\n",
    "plt.xlabel(\"bill_length_mm (scaled)\")\n",
    "plt.ylabel(\"body_mass_g (scaled)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "### Gaussian mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180",
   "metadata": {},
   "source": [
    "The Gaussian Mixture Model **optimizes the parameters of a finite number of Gaussians** to fit the data.\n",
    "\n",
    "Documentation : https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of Gaussian Mixture model with 3 components\n",
    "gmm = ...\n",
    "\n",
    "# Fit to the data\n",
    "...\n",
    "\n",
    "# Clusters prediction\n",
    "gmm_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=penguins_labels, marker='o')\n",
    "plt.scatter(penguins_X[:, 0], penguins_X[:, 1], c=gmm_labels, marker='*')\n",
    "\n",
    "\n",
    "plt.xlabel(\"bill_length_mm (scaled)\")\n",
    "plt.ylabel(\"body_mass_g (scaled)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette coefficient for GMM (k=3) : %.2f\" % metrics.silhouette_score(penguins_X, gmm_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Adjusted Rand Index for GMM (K=3) : %.2f\" % metrics.adjusted_rand_score(penguins_labels, gmm_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
