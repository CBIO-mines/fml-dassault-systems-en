{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 5 - Optimization and neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Optimisation\n",
    "\n",
    "First, we will talk about optimization in general and its application to machine learning.\n",
    "\n",
    "First we will look into a general setting. Let us simply minimize the function :\n",
    " $ f(x) = x^2 $ when starting from $x_0=2$\n",
    " \n",
    " A one-liner for that is to use scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "\n",
    "x_0 = 2\n",
    "\n",
    "result = scipy.optimize.minimize(f, x_0)\n",
    "print(result.x)  # this should print a value very close to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Implementing a random search\n",
    "\n",
    "A first possible algorithm is to sample a change for x and keep the best value.\n",
    "We iterate the following steps : \n",
    "- take a neighbor for x, sampling a random number with standard variation 0.01.\n",
    "- evaluate these two possibilities\n",
    "- move to the best one\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "x = x_0\n",
    "all_results = list()\n",
    "\n",
    "\n",
    "def sample_around(x):\n",
    "    return x + np.random.normal(scale=0.01)\n",
    "\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    sample = sample_around(x)\n",
    "    f_x, f_sample = f(x), f(sample)\n",
    "    # The best value is the sampled one\n",
    "    if f_sample < f_x:\n",
    "        x = sample\n",
    "        all_results.append(f_sample)\n",
    "    # The best one is the former one\n",
    "    else:\n",
    "        x = x\n",
    "        all_results.append(f_x)\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Implementing an exaustive search\n",
    "\n",
    "A first possible algorithm is to try all changes for x and keep the best value.\n",
    "We iterate the following steps : \n",
    "- try a smaller and a larger x value of 0.01.\n",
    "- evaluate these two possibilities\n",
    "- move to the best one\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "x = x_0\n",
    "all_results = list()\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    smaller, larger = x - 0.01, x + 0.01\n",
    "    # ...\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Implementing a gradient descent 'by hand'\n",
    "Now let us implement the gradient descent, by remembering that $\\frac{df}{dx} = 2x$\n",
    "\n",
    "We iterate the following steps : \n",
    "- compute the gradient value at x\n",
    "- Update x : $x \\leftarrow x - 0.01 \\frac{df}{dx}$\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "# Implement the learning loop\n",
    "# ...\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Implementing a gradient descent with automatic differentiation (by hand)\n",
    "\n",
    "We want to use the same algorithm but without knowing the formula of differentiation. \n",
    "We instead want to rely on Pytorch\n",
    "\n",
    "Below is the implementation of the same method as before, with PyTorch.\n",
    " \n",
    "Can you confirm that we get the same results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    f_x = x ** 2\n",
    "    f_x.backward()\n",
    "    x.data = x - 0.01 * x.grad.item()\n",
    "    x.grad = None\n",
    "    all_results.append(f_x.data)\n",
    "\n",
    "print(x.item())\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Implementing a gradient descent with automatic differentiation (the proper way)\n",
    "\n",
    "You don't have anything to do for this part, this is just showing you the proper PyTorch syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    f_x = f(x)\n",
    "    f_x.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    all_results.append(f_x.data)\n",
    "\n",
    "print(x.item())\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Bigger input space\n",
    "\n",
    "Let us now look at a more complicated input space, the function takes as input five numbers and returns :\n",
    "$f_2(x_1, x_2, x_3, x_4, x_5) = (x_1 + x_2 + x_3 + x_4 + x_5)^2$\n",
    "\n",
    "Now it is more costly to find the right direction randomly. Try the random algorithm on this new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_2(x):\n",
    "    return (x[0] + x[1] + x[2] + x[3] + x[4]) ** 2\n",
    "\n",
    "\n",
    "new_x_0 = (1, 2, 3, 4, 5)\n",
    "f_2(new_x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "x = new_x_0\n",
    "all_results = list()\n",
    "\n",
    "\n",
    "def sample_around(x):\n",
    "    return x + np.random.normal(size=5, scale=0.01)\n",
    "\n",
    "\n",
    "# Adapt the random algorithm defined above to this new problem\n",
    "# ...\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Now let us try the gradient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(new_x_0, requires_grad=True, dtype=float)\n",
    "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
    "\n",
    "# Adapt the PyTorch SGD algorithm defined above to this new problem\n",
    "# ...\n",
    "\n",
    "print(x.data)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "What can you say about those two different methods ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Actual machine learning examples\n",
    "\n",
    "Now instead of minimizing random functions, let us minimize the error of a linear model !\n",
    "\n",
    "We will use generated data (that I used during my class) : we simulate a hidden relationship (base_function) by sampling input-output pairs with noise.\n",
    "\n",
    "Let us generate the data once again and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def base_function(x):\n",
    "    y = 1.3 * x ** 3 - 3 * x ** 2 + 3.6 * x + 6.9\n",
    "    return y\n",
    "\n",
    "\n",
    "low, high = -1, 3\n",
    "n_points = 80\n",
    "\n",
    "# Get the values\n",
    "xs = np.random.uniform(low, high, n_points)[:, None]\n",
    "sample_ys = base_function(xs)\n",
    "ys_noise = np.random.normal(size=(len(xs), 1))\n",
    "noisy_sample_ys = sample_ys + ys_noise\n",
    "\n",
    "# Plot the hidden function\n",
    "lsp = np.linspace(low, high)[:, None]\n",
    "true_ys = base_function(lsp)\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Gradient descent using torch.\n",
    "First create a torch version of these objects. \n",
    "\n",
    "We specify a float32 dtype for our objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_noisy_sample_ys = torch.from_numpy(noisy_sample_ys).float()\n",
    "torch_xs = torch.from_numpy(xs).float()\n",
    "torch_lsp = torch.from_numpy(lsp).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Let us try to fit a linear model by hand, instead of simply relying on scikit-learn !\n",
    "\n",
    "The model of a linear regression is : $f_\\theta (x) = (\\theta_1 x + \\theta_0)$\n",
    "\n",
    "Careful ! We do not want to minimize the function of x itself. \n",
    "\n",
    "We want to minimise the errors we make, also called the loss function. We will do this by adjusting the parameters $\\theta$ of the function, starting from an arbitrary value of (1,1). This loss function is the sum of the square errors at each point : \n",
    "\n",
    "$$ \\min_{\\theta}\\mathcal{L} (\\theta) = 1/N\\sum_i (y_i - f_{\\theta} (x_i))^ 2 \\\\\n",
    "= 1/N\\sum_i (y_i - (\\theta_1 x_i + \\theta_0))^ 2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_theta(x, theta):\n",
    "    return theta[1] * x + theta[0]\n",
    "\n",
    "\n",
    "def loss_function(theta):\n",
    "    return torch.mean((torch_noisy_sample_ys - f_theta(torch_xs, theta)) ** 2)\n",
    "\n",
    "\n",
    "initial_theta = torch.tensor((1., 1.), requires_grad=True)\n",
    "initial_loss = loss_function(initial_theta)\n",
    "print(initial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "theta = copy.deepcopy(initial_theta)\n",
    "opt = torch.optim.SGD([theta], lr=0.01, momentum=0.0)\n",
    "\n",
    "# Implement optimization here, as we did above\n",
    "# ...\n",
    "\n",
    "print(theta.data)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We have values for the parameters now. \n",
    "Let us look at what they look like.\n",
    "\n",
    "Use the learnt function on the linspace to plot your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ys = f_theta(torch_lsp, theta).detach().numpy()\n",
    "\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "plt.plot(lsp, predicted_ys)\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Deep Learning with PyTorch\n",
    "\n",
    "We start by training a small MLP using built-in functionalities in scikit-learn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_model = MLPRegressor(max_iter=5000)\n",
    "mlp_model.fit(xs, noisy_sample_ys.flatten())\n",
    "predicted_lsp = mlp_model.predict(lsp)\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.plot(lsp, predicted_lsp, color='orange', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "MLPRegressor works well for this simple data, but it lacks the more advanced deep learning modeling that PyTorch can offer.\n",
    "Let's start by achieving a similar result to MLPRegressor, but defining our model ourselves and in PyTorch.\n",
    "\n",
    "By default, the MLP Regressor makes the following computational graph :\n",
    "- input gets multiplied by a matrix with 100 parameters, and an additional parameter is added to each values, giving 100 outputs y (shape = (n_samples, 100))\n",
    "- ReLU is applied to each of these outputs (shape = (n_samples, 100)). The relu function is implemented in PyTorch with torch.nn.functional.relu(x)\n",
    "- Then this value is multiplied by a matrix to produce a scalar output (again 100 parameters) (shape = (n_samples, 1)) and shifted by an offset.\n",
    "\n",
    "A quick reminder on matrix multiplication : it is an operation that combines one matrix A of shape (m,n) and a matrix B of shape (n,p) into a matrix C of shape (m,p). In PyTorch (and NumPy), you need to call torch.matmul(A,B) to make this computation.\n",
    "\n",
    "To make the two big multiplications, we will use one torch tensor of 100 parameters for each multiplication, with the appropriate shape.Create random starting tensors of parameters.\n",
    "\n",
    "Then implement the asked computation to produce our output from our input. You should debug the operations by ensuring the shapes are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the parameters with small random initial values.\n",
    "# We need to mention that we want to compute a gradient\n",
    "# We provide you with the example for the first one, fill the others :\n",
    "w1 = torch.normal(mean=0., std=0.1, size=(1, 100), requires_grad=True)\n",
    "b1 = 0\n",
    "w2 = 0\n",
    "b2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then define the function\n",
    "def f(x, weight1=w1, bias1=b1, weight2=w2, bias2=b2):\n",
    "    out = torch.tensor(0)  # Replace this with the actual output\n",
    "    return out\n",
    "\n",
    "\n",
    "# Check that when doing inference on the data, we get an output tensor of shape (80,1) that corresponds to 80 predictions.\n",
    "f(torch_xs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Now we will mostly use the optimization procedure above to train our network using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now like last time, let us define an optimizer and give the parameters to it.\n",
    "n_iter = 2000\n",
    "opt = torch.optim.Adam([w1, b1, w2, b2], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the data, make the forward, backward, step and zero grad\n",
    "for i in range(n_iter):\n",
    "    \n",
    "    # Replace with actual computations\n",
    "    # ...\n",
    "    loss = 0\n",
    "    if not i % 10:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ys = f(torch_lsp).detach().numpy()\n",
    "\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "plt.plot(lsp, predicted_ys)\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Congratulations, you have coded yourself a MLP model ! We have used the computation graph framework.\n",
    "\n",
    "\n",
    "Now let us make our code prettier (more Pytorch) and more efficient.\n",
    "First let us refactor the model in the proper way it should be coded, by using the [`torch.nn.Module`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html) class.\n",
    "You should add almost no new code, just reorganize the one above into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "\n",
    "class MyOwnMLP(Module):\n",
    "    def __init__(self):\n",
    "        super(MyOwnMLP, self).__init__()\n",
    "        self.w1 = Parameter(torch.normal(mean=0., std=0.1, size=(1, 100)))\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass (computation graph) like above\n",
    "        return out\n",
    "\n",
    "\n",
    "model = MyOwnMLP()\n",
    "out = model(torch_xs)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Now we are good to also make the data iteration process look like Pytorch code !\n",
    "\n",
    "We need to define a Dataset object. Once we have this, we can use it to create a DataLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        pass\n",
    "        # store the data\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "        # Return the number of points in your dataset\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the x and y at a given position (index) in the data\n",
    "        x = 0\n",
    "        y = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop and wait for each data point in PyTorch\n",
    "dataset = CustomDataset(data_x=torch_xs, data_y=torch_noisy_sample_ys)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=10, num_workers=0)\n",
    "start = time.time()\n",
    "for point in dataloader:\n",
    "    pass\n",
    "print('Done in pytorch : ', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "The last thing missing to make our pipeline truly Pytorch is to use a GPU.\n",
    "\n",
    "In Pytorch it is really easy, you just need to 'move' your tensors to a 'device'.\n",
    "You can test if a gpu is available and create the appropriate device with the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
    "torch_xs = torch_xs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Now we finally have all the elements to make an actual Pytorch complete pipeline !\n",
    "\n",
    "Create a model, and try to put it on a device.\n",
    "Create an optimizer with your model's parameters\n",
    "Make your data into a dataloader\n",
    "\n",
    "Then use two nested for loops : one for 100 epochs, and in each epoch loop over the dataloader\n",
    "    Inside the loop, for every batch first put the data on the device\n",
    "    Then use the semantics of above :\n",
    "        - model(batch)\n",
    "        - loss computation and backward\n",
    "        - gradient step and zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Don't forget to send to device, the rest is similar to what we had above\n",
    "        pass\n",
    "\n",
    "# To easily use the trained model we need to send it back to cpu at the end\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "We finally can plot the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ys = model(torch_lsp).detach().numpy()\n",
    "\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "plt.plot(lsp, predicted_ys)\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "This is the end of the practical part of training neural networks !\n",
    "\n",
    "Of course, a lot more can be done. On this simple toy data, you can try to illustrate concepts of this class:\n",
    "- What happens if you use only 10 data points and increase the noise level ?\n",
    "- Can you observe an overfitting behavior ?\n",
    "- Can you see the impact of using different optimisers (SGD vs Adam) ?\n",
    "- ...\n",
    "\n",
    "Another interesting extension is to use a more advanced (yet manageable dataset), such as FashionMnist.\n",
    "You can use it through the built-in PyTorch objects: _torchvision.datasets.FashionMNIST_ . \n",
    "You can install torchvision with _pip install torchvision_ .\n",
    "More generally, you can follow this tutorial: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html to access the data and have a first model example and training:\n",
    "- Can you compare MLP architectures with CNNs on this task ?\n",
    "- Do you see an overfit on this dataset ?\n",
    "- Does data augmentation helps training on this dataset ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
