{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 1 : Getting familiar with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "Notebook prepared by [ChloÃ©-Agathe Azencott](http://cazencott.info), modified by [Victor Laigle](https://eaglev-sci.github.io/).\n",
    "\n",
    "This notebook will allow you to discover `scikit-learn` functionalities to:\n",
    "* train and evaluate a supervised learning algorithm\n",
    "* scale variables to a range of values\n",
    "* transform variables to make their distribution closer to a Gaussian distribution.\n",
    "* encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages numpy, pandas and matplotlib (with aliases np, pd and plt respectively)\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', **{'size': 12}) # set the global font size for plots (in pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "In this notebook, we will work with the data contained in the `data/auto-mpg.tsv` file. This data, obtained from https://archive.ics.uci.edu/ml/datasets/Auto+MPG, describes cars using the following variables:\n",
    "\n",
    "    1. mpg:           continuous (consumption in miles per gallon)\n",
    "    2. cylinders:     discrete (number of cylinders)\n",
    "    3. displacement:  continuous (air volume moved by the pistons in the engine)\n",
    "    4. horsepower:    continuous\n",
    "    5. weight:        continuous\n",
    "    6. acceleration:  continuous\n",
    "    7. model year:    discrete\n",
    "    8. origin:        discrete (region of origin, 1=North America, 2=Europe, 3=Asia)\n",
    "    9. car name:      string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Our goal is to predict the consumption of a vehicle (mpg) from the other variables (excluding the car name, which is a unique identifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We will start by downloading the data from the github repository to your current working directory, and load the data into a `pandas` `dataframe`. \n",
    "\n",
    "If you already downloaded the data, you can use the alternative suggested in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data to the current directory\n",
    "!wget https://raw.githubusercontent.com/CBIO-mines/fml-dassault-systems-en/main/data/auto-mpg.tsv\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"auto-mpg.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "__Alternatively :__ If you're not on Colab and have already downloaded the file to a `data` folder, uncomment and run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"data/auto-mpg.tsv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the data looks like with the first 10 rows of the dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Create data matrices X and y\n",
    "\n",
    "- X: predictive variables\n",
    "- y: true values (\"ground truth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(columns=['mpg', 'car name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 2. Data Visualization\n",
    "\n",
    "We will now visualize the variables representing our vehicles. To do this, we will separate the continuous variables (which we will represent each with a histogram) from the discrete variables (which we will represent with bar charts).\n",
    "\n",
    "Feel free to adjust the parameters of the `matplotlib` methods to produce more readable plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continuous and discrete features\n",
    "continuous_features = ['displacement', 'horsepower', 'weight', 'acceleration']\n",
    "discrete_features = ['cylinders', 'model year', 'origin']\n",
    "\n",
    "# Get feature names and their indices in the dataframe\n",
    "features = list(df.drop(columns=['mpg', 'car name']).columns)\n",
    "\n",
    "continuous_features_idx = [features.index(feat_name) for feat_name in continuous_features]\n",
    "discrete_features_idx = [features.index(feat_name) for feat_name in discrete_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Histograms for the continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "for (plot_idx, feat_idx) in enumerate(continuous_features_idx):\n",
    "    # Create a graph at position (plot_idx+1) of a 2x2 grid\n",
    "    ax = fig.add_subplot(2, 2, (plot_idx+1))\n",
    "    # Display histogram for the variable at index feat_idx\n",
    "    h = ax.hist(X[:, feat_idx], bins=30, edgecolor='none')\n",
    "    # Use variable name as title\n",
    "    ax.set_title(features[feat_idx])\n",
    "# Handle spacing between graphs\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Bar charts for the discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "\n",
    "for (plot_idx, feat_idx) in enumerate(discrete_features_idx):\n",
    "    # Create a graph at position (plot_idx+1) of a 1x3 grid\n",
    "    ax = fig.add_subplot(1, 3, (plot_idx+1))\n",
    "\n",
    "    # Compute frequencies of each value of the variable at index feat_idx\n",
    "    feature_values = np.unique(X[:, feat_idx])\n",
    "    frequencies = [(float(len(np.where(X[:, feat_idx]==value)[0]))/X.shape[0]) \\\n",
    "                   for value in feature_values]\n",
    "    \n",
    "    # Display bar chart for the variable at index feat_idx\n",
    "    b = ax.bar(range(len(feature_values)), frequencies, width=0.5, \n",
    "               tick_label=list([int(n) for n in feature_values]))\n",
    "    \n",
    "    # Set variable name as title\n",
    "    ax.set_title(features[feat_idx])\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "__Question:__ Observe the orders of magnitude / value ranges of the different variables. What can you comment about them ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Histogram of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y, bins=30, edgecolor='none')\n",
    "plt.title('mpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Linear Regression\n",
    "\n",
    "We will now use `scikit-learn` to train a linear regression on the data.\n",
    "\n",
    "The linear models in `scikit-learn` are implemented in the [`sklearn.linear_model`](https://scikit-learn.org/stable/api/sklearn.linear_model.html) module.\n",
    "\n",
    "__Feel free to refer frequently to the [scikit-learn documentation](https://scikit-learn.org/stable/api/index.html#), which is very comprehensive.__\n",
    "\n",
    "The framework, which is very common, is the following:\n",
    "- initialize a model object\n",
    "- train the model on the data\n",
    "- use the model to predict new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a LinearRegression object\n",
    "predictor = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train this object on the data\n",
    "predictor.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "\n",
    "We can now use this model to __predict__ labels from the variables. Particularly, it's possible to apply it to the data we've just used to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predictor.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "__CAUTION__ In practice, what we are really interested in is a model's ability to make good predictions on data that was __not__ used to train it. A model's performance on the data used to train it does not allow to determine whether it is a good model. We will discuss this in more detail later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "We would now like to evaluate our model.\n",
    "\n",
    "To do this, we will use the functionalities of the [`metrics`](https://scikit-learn.org/stable/api/sklearn.metrics.html) module from `scikit-learn`.\n",
    "\n",
    "As this is a regression problem, we will use the __RMSE__ (_Root Mean Squared Error_) as a measure of the model's performance: this is the square root of the mean of the squared errors. The square root is used for homogeneity reasons: the RMSE is expressed in the same unit as the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE: %.2f\" % metrics.root_mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "__Question:__ What do you think about this error ? Is it high ? Low ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "We can also use a visual, and represent each individual from the test set by its predicted label vs.its true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(y, y_pred)\n",
    "\n",
    "plt.xlabel(\"Real Consumption (mpg)\")\n",
    "plt.ylabel(\"Predicted Consumption (mpg)\")\n",
    "plt.title(\"Linear Regression\")\n",
    "\n",
    "# Same values on both axes\n",
    "axis_min = np.min([np.min(y), np.min(y_pred)])-1\n",
    "axis_max = np.max([np.max(y), np.max(y_pred)])+1\n",
    "plt.xlim(axis_min, axis_max)\n",
    "plt.ylim(axis_min, axis_max)\n",
    "  \n",
    "# Diagonal y=x\n",
    "plt.plot([axis_min, axis_max], [axis_min, axis_max], 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "### Regression coefficients\n",
    "\n",
    "To understand our model, we can look at the coefficients attributed to each variable in the learnt linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display, for each variable, the absolute value of its coefficient in the model\n",
    "num_features = X.shape[1]\n",
    "feature_names = df.drop(columns=['mpg', 'car name']).columns\n",
    "plt.scatter(range(num_features), np.abs(predictor.coef_))\n",
    "\n",
    "plt.xlabel('Variable')\n",
    "tmp = plt.xticks(range(num_features), feature_names, rotation=90)\n",
    "tmp = plt.ylabel('Coefficient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "__Question:__ Which variable has the highest coefficient (in absolute value) ? Do you think that it means this variable plays a very important part in the prediction ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## 4. Changing the scale of the variables\n",
    "\n",
    "The fact that variables are on different scales makes the interpretation of the linear regression coefficients quite tricky."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "### Variables transformation\n",
    "\n",
    "Centering (bringing to a mean of 0) and scaling (bringing to a standard deviation of 1) the variables helps to remedy this problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X.shape\n",
    "X_scaled_manual = np.zeros_like(X)  # Create an array with same shape as X to hold the standardized variables\n",
    "\n",
    "# Compute the mean and standard deviation of each variable in X, and use them to standardize the variable\n",
    "# Values from column i in the array X can be accessed with X[:, i]\n",
    "for i in range(n_features):\n",
    "    \n",
    "    # START OF YOUR CODE\n",
    "    \n",
    "    # Compute the mean\n",
    "    mean = sum(X[:, i]) / n_samples\n",
    "    # Compute the standard deviation\n",
    "    variance = sum((X[:, i] - mean) ** 2) / n_samples\n",
    "    std_dev = variance ** 0.5\n",
    "    # Standardize the variable\n",
    "    X_scaled_manual[:, i] = (X[:, i] - mean) / std_dev\n",
    "\n",
    "    # END OF YOUR CODE\n",
    "\n",
    "# The whole loop can be written more concisely using numpy functions as follows:\n",
    "X_scaled_numpy = (X - np.mean(X, axis=0)) / np.std(X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original data (first 5 rows and 5 columns):\")\n",
    "print(X[0:5, 0:5])\n",
    "\n",
    "print(\"\\nStandardized data (first 5 rows and 5 columns):\")\n",
    "print(X_scaled_manual[0:5, 0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Check that the means and standard deviations of your variables are indeed all set to 0 and 1 (or values very close to these, due to numeric approximations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Means: {[f'{x:.3f}' for x in X_scaled_manual.mean(axis=0)]}\")\n",
    "print(f\"Std: {[f'{x:.3f}' for x in X_scaled_manual.std(axis=0)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Now that we've seen how it works, we can use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) object of the [`sklearn.preprocessing`](https://scikit-learn.org/stable/api/sklearn.preprocessing.html) module to do it for us automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_scaler = preprocessing.StandardScaler()\n",
    "standard_scaler.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = standard_scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Note the use of the `transform` method here: the `fit` function is used to compute the values needed to center and scale the data in X, but it does not transform the data itself. We need to specifically ask to *transform* the data. Among other things, it allows to keep our original data `X` unchanged if we need to. There also exists a [`fit_transform`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler.fit_transform) method to do both in the same function call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Visualization of the new variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "#### Histograms for continuous variables\n",
    "We simply replace `X` by `X_scaled` in the code used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "for (plot_idx, feat_idx) in enumerate(continuous_features_idx):\n",
    "    # Create a graph at position (plot_idx+1) of a 2x2 grid\n",
    "    ax = fig.add_subplot(2, 2, (plot_idx+1))\n",
    "    # Display histogram for the variable at index feat_idx\n",
    "    h = ax.hist(X_scaled[:, feat_idx], bins=30, edgecolor='none')\n",
    "    # Use variable name as title\n",
    "    ax.set_title(features[feat_idx])\n",
    "# Handle spacing between graphs\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Bar charts for discrete variables\n",
    "\n",
    "Same here, we replace `X` by `X_scaled` in the previous code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "\n",
    "for (plot_idx, feat_idx) in enumerate(discrete_features_idx):\n",
    "    # Create a graph at position (plot_idx+1) of a 1x3 grid\n",
    "    ax = fig.add_subplot(1, 3, (plot_idx+1))\n",
    "\n",
    "    # Compute frequencies of each value of the variable at index feat_idx\n",
    "    feature_values = np.unique(X_scaled[:, feat_idx])\n",
    "    frequencies = [(float(len(np.where(X_scaled[:, feat_idx]==value)[0]))/X_scaled.shape[0]) \\\n",
    "                   for value in feature_values]\n",
    "    \n",
    "    # Display bar chart for the variable at index feat_idx\n",
    "    b = ax.bar(range(len(feature_values)), frequencies, width=0.5, \n",
    "               tick_label=list(['%.1f' % n for n in feature_values]))\n",
    "    \n",
    "    # Set variable name as title\n",
    "    ax.set_title(features[feat_idx])\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "### Impact on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "We can now train a model `predictor_scaled` on the centered and scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START OF YOUR CODE\n",
    "\n",
    "# Create a new LinearRegression object\n",
    "predictor_scaled = linear_model.LinearRegression()\n",
    "\n",
    "# Train predictor_scaled on the new data\n",
    "predictor_scaled.fit(X_scaled, y)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "And create an array `y_pred_scaled` which contains the predictions of `predictor_scaled` on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START OF YOUR CODE\n",
    "\n",
    "y_pred_scaled = predictor_scaled.predict(X_scaled)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "The RMSE of this new model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE (scaled): %.2f\" % metrics.root_mean_squared_error(y, y_pred_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "__Question:__ Compare it to the previous RMSE. Are the predictions any different ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(y_pred_scaled, y_pred)\n",
    "\n",
    "plt.xlabel(\"Comsumption predicted on scaled data (mpg)\")\n",
    "plt.ylabel(\"Comsumption predicted from original data (mpg)\")\n",
    "plt.title(\"Comparison of linear regression predictions\")\n",
    "\n",
    "# Same values on both axes\n",
    "axis_min = np.min([np.min(y), np.min(y_pred)])-1\n",
    "axis_max = np.max([np.max(y), np.max(y_pred)])+1\n",
    "plt.xlim(axis_min, axis_max)\n",
    "plt.ylim(axis_min, axis_max)\n",
    "  \n",
    "# Diagonal y=x\n",
    "plt.plot([axis_min, axis_max], [axis_min, axis_max], 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "#### Comparison of regression coefficients\n",
    "\n",
    "Finally, we can compare the regression coefficients from both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display, for each variable, the absolute value of its coefficient in the model\n",
    "num_features = X.shape[1]\n",
    "plt.scatter(range(num_features), np.abs(predictor.coef_), label='Original')\n",
    "\n",
    "plt.scatter(range(num_features), np.abs(predictor_scaled.coef_), label='Scaled', marker='x')\n",
    "\n",
    "plt.xlabel('Variable')\n",
    "tmp = plt.xticks(range(num_features), feature_names, rotation=90)\n",
    "tmp = plt.ylabel('Coefficient')\n",
    "plt.legend(loc=(0.02, 0.75))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "We can notice that, even if the RMSE remains identical, standardizing (= centering and scaling) the variables changed the values of the parameters learnt by the model. We can compare for example the values taken by the intercept (independent term in the linear model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intercept in the two models\")\n",
    "print(f\"- Original data: {predictor.intercept_:.3f}\")\n",
    "print(f\"- Scaled data  : {predictor_scaled.intercept_:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "__Question:__ Which variables are now the most relevant to predict a vehicle's consumption ? Does it seem sensible ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Encoding categorical variables\n",
    "\n",
    "The `origin` variable is a qualitative (or categorical) variable: the 1-2-3 encoding is completely arbitrary. It implies, in particular, if we think in terms of distances, that Asia is twice as far from North America as it is from Europe, which does not make sense.\n",
    "\n",
    "A more reasonable encoding for this kind of case is what is called _one-hot_, or _dummy encoding_: we represent the variable by as many binary variables as there are possible values (3 in the case of the `origin` variable: the first corresponds to North America, the second to Europe, the third to Asia), and we set to `1` the single binary variable corresponding to the value we are encoding.\n",
    "\n",
    "Thus the single `origin` variable becomes 3 binary variables:\n",
    "```\n",
    "   North America --> 1, 0, 0\n",
    "   Europe --> 0, 1, 0\n",
    "   Asia --> 0, 0, 1\n",
    "```\n",
    "\n",
    "This representation has the disadvantage of increasing the number of variables, but the Euclidean distances are now more reasonable (they are 1 if the values are different and 0 if they are identical)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "This functionality exists in `pandas` as well as in `scikit-learn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### One-hot transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe where the 'origin' column is replaced by its 'one-hot' encoding\n",
    "df_dummies = pd.get_dummies(df, columns=['origin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data once again\n",
    "X_dummies = np.array(df_dummies.drop(columns=['mpg', 'car name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "As previously, we normalize each of the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START OF YOUR CODE\n",
    "\n",
    "# Initialize a StandardScaler object\n",
    "standard_scaler_dummies = preprocessing.StandardScaler()\n",
    "# Fit the object on the data with dummies\n",
    "standard_scaler_dummies.fit(X_dummies)\n",
    "# Scale the data with dummies\n",
    "X_scaled_dummies = standard_scaler_dummies.transform(X_dummies)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### Impact on the model\n",
    "\n",
    "Let's learn a linear regression on the data where the `origin` varibale has been replaced by its one-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "To do so, we create an instance `predictor_dummy` from the `LinearRegression` class, trained on the data containing the _one-hot_ version of the `origin` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START OF YOUR CODE\n",
    "\n",
    "# Create a new LinearRegression object\n",
    "predictor_dummy = linear_model.LinearRegression()\n",
    "\n",
    "# Train predictor_dummy on the new data\n",
    "predictor_dummy.fit(X_scaled_dummies, y)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "We can now create an array `y_pred_dummy` which contains the predictions of the new linear regression on these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# START OF YOUR CODE\n",
    "\n",
    "y_pred_dummy = predictor_dummy.predict(X_scaled_dummies)\n",
    "\n",
    "# END OF YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "RMSE for this new model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE (one-hot encoding): %.2f\" % metrics.root_mean_squared_error(y, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "__Question:__ Compare it to the previous RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Comparison to previous predictions\n",
    "\n",
    "Are the performances really different ? We can compare the predictions directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "plt.scatter(y_pred, y_pred_dummy)\n",
    "\n",
    "plt.xlabel(\"Predicted consumption (mpg) (baseline)\")\n",
    "plt.ylabel(\"Predicted consumption (mpg) (one-hot)\")\n",
    "plt.title(\"Comparison of Linear Regression predictions\")\n",
    "\n",
    "# Same values on both axes\n",
    "axis_min = np.min([np.min(y_pred), np.min(y_pred_dummy)])-1\n",
    "axis_max = np.max([np.max(y_pred), np.max(y_pred_dummy)])+1\n",
    "plt.xlim(axis_min, axis_max)\n",
    "plt.ylim(axis_min, axis_max)\n",
    "  \n",
    "# Diagonal y=x\n",
    "plt.plot([axis_min, axis_max], [axis_min, axis_max], 'k-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "Let's see what the correlation is between the two predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, pval = st.pearsonr(y_pred, y_pred_dummy)\n",
    "print(\"Correlation between predictions : %.2f (p=%.2e)\" % (r, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "#### Comparison of the regression coefficients\n",
    "\n",
    "Let's now compare visually the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display for each variable, the absolute value of its coefficient in the model\n",
    "num_features = X.shape[1]\n",
    "plt.scatter(range(num_features), np.abs(predictor_scaled.coef_), label='Scaled')\n",
    "\n",
    "num_features2 = X_scaled_dummies.shape[1]\n",
    "plt.scatter(range(num_features2), np.abs(predictor_dummy.coef_), label='One-hot', marker='x')\n",
    "feature_names2 = df_dummies.drop(columns=['mpg', 'car name']).columns\n",
    "\n",
    "plt.xlabel('Variable')\n",
    "tmp = plt.xticks(range(num_features2), feature_names2, rotation=90)\n",
    "tmp = plt.ylabel('Coefficient')\n",
    "\n",
    "plt.legend(loc=(0.02, 0.75))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We reached the end of this practical. Here is a summary of what we have covered, with the key takeaways:\n",
    "- We used the `scikit-learn` library to predict a quantitative, continuous variable (car consumption, mpg) from continuous and discrete variables (cars features)\n",
    "\n",
    "- `scikit-learn` uses a general framework with\n",
    "  - object initialization\n",
    "  - Training the object on the data with `fit`\n",
    "  - Predicting the output values with `predict` or transforming the data with `transform`\n",
    "\n",
    "- We tried a first predictive model: the linear regression from [`sklearn.linear_model`](https://scikit-learn.org/stable/api/sklearn.linear_model.html)\n",
    "\n",
    "- **Scaling** the data so that every variable has some similar range of values is (very) important.\n",
    "  - For the linear regression model here, it allowed a much better interpretation\n",
    "  - With other models, scaling the variables might be critical for the model to learn anything\n",
    "  - Especially, it prevents the model to attribute too much importance to a variable solely because of its values range\n",
    "\n",
    "- We've used an example of `scikit-learn`'s object to scale the data: the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "- Categorical variables can be encoded with a _one-hot encoding_, which avoids arbitrary order or distances between the variables. A drawback of this is that we need to increase the number of variables used by the model, so it is often a trade-off between these two aspects. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
