{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Notebook 5 - Optimization and neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "executionInfo": {
     "elapsed": 9739,
     "status": "ok",
     "timestamp": 1745571060452,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "1"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "source": [
    "## Optimization\n",
    "\n",
    "In this session we will talk about optimization in general and its application to machine learning.\n",
    "\n",
    "First we will look into a general setting. Let us simply minimize the function :\n",
    " $ f(x) = x^2 $ when starting from $x_0=2$\n",
    "\n",
    " A one-liner for that is to use scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1745571062011,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "3",
    "outputId": "bc894f2b-ab75-4622-a9f5-ba86f7b1a8d8"
   },
   "outputs": [],
   "source": [
    "# Define function f(x) which returns x squared\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "\n",
    "# Define an initial value to start the optimisation\n",
    "x_0 = 2\n",
    "\n",
    "# Use the 'minimize' function to find the value of x that minimizes f(x)\n",
    "# The function starts at x_0 and searches for the minimum\n",
    "result = scipy.optimize.minimize(f, x_0)\n",
    "\n",
    "# Extract the value of x that minimizes the function\n",
    "# It should be close to zero for this function\n",
    "result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "4"
   },
   "source": [
    "### Implementing a random search\n",
    "\n",
    "A first possible algorithm is to sample a change for x and keep the best value.\n",
    "We iterate the following steps :\n",
    "- take a neighbor for x, sampling a random number with standard variation 0.01.\n",
    "- evaluate these two possibilities\n",
    "- move to the best one\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1745564155601,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "5",
    "outputId": "570cebdc-4f4b-496c-e3e1-96677687ed3c"
   },
   "outputs": [],
   "source": [
    "# Define the number of iterations for the optimization algorithm\n",
    "n_iter = 1000\n",
    "\n",
    "# Initialize x with the initial value x_0\n",
    "x = x_0\n",
    "\n",
    "# Create a list to store all the results of the function over the iterations\n",
    "all_results = list()\n",
    "\n",
    "# Define a function that samples around the current value of x\n",
    "# It adds Gaussian noise with a standard deviation of 0.01\n",
    "def sample_around(x):\n",
    "    return x + np.random.normal(scale=0.01)\n",
    "\n",
    "# Loop over the specified number of iterations\n",
    "for _ in range(n_iter):\n",
    "    # Sample around the current value of x\n",
    "    sample = sample_around(x)\n",
    "\n",
    "    # Calculate the function values for x and the sample\n",
    "    f_x, f_sample = f(x), f(sample)\n",
    "\n",
    "    # If the function value for the sample is lower than that of x\n",
    "    # then update x with the sample value\n",
    "    if f_sample < f_x:\n",
    "        x = sample\n",
    "        all_results.append(f_sample)\n",
    "    # Otherwise, keep the current value of x\n",
    "    else:\n",
    "        x = x\n",
    "        all_results.append(f_x)\n",
    "\n",
    "# Print the final value of x after all iterations\n",
    "print(x)\n",
    "\n",
    "# Plot the function values over the iterations\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "6"
   },
   "source": [
    "### Implementing an exaustive search\n",
    "\n",
    "A first possible algorithm is to try all changes for x and keep the best value.\n",
    "We iterate the following steps :\n",
    "- try a smaller and a larger x value of 0.01.\n",
    "- evaluate these two possibilities\n",
    "- move to the best one\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1745564156852,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "7",
    "outputId": "501043de-bda4-42ed-a311-7be7e02370cb"
   },
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "x = x_0\n",
    "all_results = list()\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    # Compute two new values around x: one smaller and one larger\n",
    "    smaller, larger = x - 0.01, x + 0.01\n",
    "\n",
    "    # Compute the function values for these two new values\n",
    "    f_small, f_large = f(smaller), f(larger)\n",
    "\n",
    "    # If the function value for the smaller value is less than that of the larger value\n",
    "    # then update x with the smaller value\n",
    "    if f_small < f_large:\n",
    "        x = smaller\n",
    "        all_results.append(f_small)\n",
    "    # Otherwise, update x with the larger value\n",
    "    else:\n",
    "        x = larger\n",
    "        all_results.append(f_large)\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "8"
   },
   "source": [
    "### Implementing a gradient descent 'by hand'\n",
    "Now let us implement the gradient descent, by remembering that $\\frac{df}{dx} = 2x$\n",
    "\n",
    "We iterate the following steps :\n",
    "- compute the gradient value at x\n",
    "- Update x : $x \\leftarrow x - 0.01 \\frac{df}{dx}$\n",
    "\n",
    "Implement that with a for loop with 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1745571092631,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "9",
    "outputId": "83a8b338-63da-4afc-ab2c-c0aceb079d4b"
   },
   "outputs": [],
   "source": [
    "# Define the derivative of the function f(x) = x^2, which is df(x) = 2x\n",
    "def df(x):\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "all_results = list()\n",
    "n_iter = 10000\n",
    "x = x_0\n",
    "for _ in range(n_iter):\n",
    "    # Compute the derivative of the function at the current position of x\n",
    "    dx = df(x)\n",
    "\n",
    "    # Update x using the gradient descent method\n",
    "    # We subtract a small multiple of the derivative to move towards the minimum\n",
    "    x = x - 0.0001 * dx\n",
    "\n",
    "    # Add the function value at the current position of x to the results list\n",
    "    all_results.append(f(x))\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "10"
   },
   "source": [
    "### Implementing a gradient descent with automatic differentiation (by hand)\n",
    "\n",
    "We want to use the same algorithm but without knowing the formula of differentiation.\n",
    "We instead want to rely on Pytorch\n",
    "\n",
    "Below is the implementation of the same method as before, with PyTorch.\n",
    "\n",
    "Can you confirm that we get the same results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 455,
     "status": "ok",
     "timestamp": 1745522186570,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "11",
    "outputId": "b0e7809c-9c6e-4fb4-e499-0140ccbb7189"
   },
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "\n",
    "# Initialize x as a PyTorch tensor with an initial value of 2.0\n",
    "# The argument requires_grad=True enables gradient's computations for this tensor\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "for i in range(n_iter):\n",
    "\n",
    "    # Compute the function value f(x) = x^2\n",
    "    f_x = x ** 2\n",
    "\n",
    "    # Compute the gradient of f_x with respect to x\n",
    "    f_x.backward()\n",
    "\n",
    "    # Update x using the gradient descent method\n",
    "    # We subtract a small multiple of the derivative to move towards the minimum\n",
    "    x.data = x - 0.01 * x.grad.item()\n",
    "\n",
    "    # Reset the gradient to None to avoid accumulation of gradients\n",
    "    x.grad = None\n",
    "\n",
    "    # Add the function value at the current position of x to the results list\n",
    "    all_results.append(f_x.data)\n",
    "\n",
    "print(x.item())\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "12"
   },
   "source": [
    "### Implementing a gradient descent with automatic differentiation (the proper way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 8102,
     "status": "ok",
     "timestamp": 1745522206037,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "13",
    "outputId": "7ca0785b-5216-45f8-8750-94eb26368b59"
   },
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Create an SGD (Stochastic Gradient Descent) optimizer with a learning rate of 0.01\n",
    "# The momentum parameter is set to 0, so it is not used here\n",
    "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Compute the function value f(x) = x^2\n",
    "    f_x = f(x)\n",
    "\n",
    "    # Compute the gradient of f_x with respect to x\n",
    "    f_x.backward()\n",
    "\n",
    "    # Update x using the SGD optimizer\n",
    "    opt.step()\n",
    "\n",
    "    # Reset gradients to zero to avoid gradients accumulation\n",
    "    opt.zero_grad()\n",
    "    all_results.append(f_x.data)\n",
    "\n",
    "print(x.item())\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "14"
   },
   "source": [
    "## Bigger input space\n",
    "\n",
    "Let us now look at a more complicated input space, the function takes as input five numbers and returns :\n",
    "$f_2(x_1, x_2, x_3, x_4, x_5) = (x_1 + x_2 + x_3 + x_4 + x_5)^2$\n",
    "\n",
    "Now it is more costly to find the right direction randomly. Try the random algorithm on this new function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1745572258575,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "15",
    "outputId": "ca4ec2fb-4439-4579-bcc9-0f618be070d9"
   },
   "outputs": [],
   "source": [
    "# Define a function f_2 that takes a vector x as input\n",
    "# and returns the square of the sum of its elements\n",
    "def f_2(x):\n",
    "    return (x[0] + x[1] + x[2] + x[3] + x[4]) ** 2\n",
    "\n",
    "\n",
    "new_x_0 = (1, 2, 3, 4, 5)\n",
    "f_2(new_x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1745572373151,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "16",
    "outputId": "c869fe74-f523-4e4f-b9f4-5c45e4f28581"
   },
   "outputs": [],
   "source": [
    "n_iter = 10000\n",
    "x = new_x_0\n",
    "all_results = list()\n",
    "\n",
    "# Define a function that samples around the current value of x\n",
    "# It adds Gaussian noise with a standard deviation of 0.01 to each component of x\n",
    "def sample_around(x):\n",
    "    return x + np.random.normal(size=5, scale=0.01)\n",
    "\n",
    "\n",
    "for _ in range(n_iter):\n",
    "    # Sample around the current value of x\n",
    "    sample = sample_around(x)\n",
    "\n",
    "    # Calculate the function values for x and the sample\n",
    "    f_x, f_sample = f_2(x), f_2(sample)\n",
    "\n",
    "    # If the function value for the sample is lower than that of x\n",
    "    # then update x with the sample value\n",
    "    if f_sample < f_x:\n",
    "        x = sample\n",
    "        all_results.append(f_sample)\n",
    "    # Otherwise, keep the current value of x\n",
    "    else:\n",
    "        x = x\n",
    "        all_results.append(f_x)\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "17"
   },
   "source": [
    "Now let us try the gradient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 592,
     "status": "ok",
     "timestamp": 1745522242097,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "18",
    "outputId": "276e92e7-f2bc-452a-fd3f-0e19f2cf388b"
   },
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "x = torch.tensor(new_x_0, requires_grad=True, dtype=float)\n",
    "opt = torch.optim.SGD([x], lr=0.01, momentum=0)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    f_x = f_2(x)\n",
    "    f_x.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "    all_results.append(f_x.data)\n",
    "\n",
    "print(x)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "19"
   },
   "source": [
    "## Actual machine learning examples\n",
    "\n",
    "\n",
    "Now instead of minimizing random functions, let us minimize the error of a linear model !\n",
    "\n",
    "We will use generated data (that I used during my class) : we simulate a hidden relationship (base_function) by sampling input-output pairs with noise.\n",
    "\n",
    "Let us generate the data once again and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1745584249858,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "20",
    "outputId": "77bad676-341e-420e-d9ea-74705544a46a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the seed for the random number generator to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the base function that we will sample\n",
    "def base_function(x):\n",
    "    y = 1.3 * x ** 3 - 3 * x ** 2 + 3.6 * x + 6.9\n",
    "    return y\n",
    "\n",
    "# Define the lower and upper bounds for the x values\n",
    "low, high = -1, 3\n",
    "\n",
    "# Define the number of points to sample\n",
    "n_points = 80\n",
    "\n",
    "# Generate random values uniformly distributed between 'low' and 'high'\n",
    "# Each value is shaped as a 2D array with a single column\n",
    "xs = np.random.uniform(low, high, n_points)[:, None]\n",
    "\n",
    "# Calculate the values of the base function for the sampled points\n",
    "sample_ys = base_function(xs)\n",
    "\n",
    "# Add Gaussian noise to the sampled values\n",
    "ys_noise = np.random.normal(size=(len(xs), 1))\n",
    "noisy_sample_ys = sample_ys + ys_noise\n",
    "\n",
    "# Create a series of linearly spaced points between 'low' and 'high'\n",
    "# Each point is shaped as a 2D array with a single column\n",
    "lsp = np.linspace(low, high)[:, None]\n",
    "\n",
    "# Compute the values of the base function for these linearly spaced points\n",
    "# These represent the true values of the function, without noise\n",
    "true_ys = base_function(lsp)\n",
    "\n",
    "# Plot the base function as a dashed line\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "\n",
    "# Plot the noisy samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "21"
   },
   "source": [
    "### Gradient descent using torch.\n",
    "First create a torch version of these objects.\n",
    "\n",
    "We specify a float32 dtype for our objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "executionInfo": {
     "elapsed": 92,
     "status": "ok",
     "timestamp": 1745584253012,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "22"
   },
   "outputs": [],
   "source": [
    "# Convert the numpy arrays 'noisy_sample_ys', 'xs' and 'lsp' to pytorch tensors of type float\n",
    "# This allows the use of PyTorch functionalities for further computations\n",
    "\n",
    "torch_noisy_sample_ys = torch.from_numpy(noisy_sample_ys).float()\n",
    "torch_xs = torch.from_numpy(xs).float()\n",
    "torch_lsp = torch.from_numpy(lsp).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {
    "id": "23"
   },
   "source": [
    "\n",
    "Let us try to fit a linear model by hand, instead of simply relying on scikit-learn !\n",
    "\n",
    "The model of a linear regression is : $f_\\theta (x) = (\\theta_1 x + \\theta_0)$\n",
    "\n",
    "Careful ! We do not want to minimize the function of x itself.\n",
    "\n",
    "We want to minimise the errors we make, also called the loss function. We will do this by adjusting the parameters $\\theta$ of the function, starting from an arbitrary value of (1,1). This loss function is the sum of the square errors at each point :\n",
    "\n",
    "$$ \\min_{\\theta}\\mathcal{L} (\\theta) = 1/N\\sum_i (y_i - f_{\\theta} (x_i))^ 2 \\\\\n",
    "= 1/N\\sum_i (y_i - (\\theta_1 x_i + \\theta_0))^ 2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1745584256685,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "24",
    "outputId": "62affe69-ac3f-4625-8abe-0949748fedf8"
   },
   "outputs": [],
   "source": [
    "# Define a function f_theta that represents a line with equation y = theta[1] * x + theta[0]\n",
    "# It takes as input a tensor x and a tensor of parameters theta\n",
    "def f_theta(x, theta):\n",
    "    return theta[1] * x + theta[0]\n",
    "\n",
    "# Define a loss function that computes the mean squared error\n",
    "# between the values predicted by f_theta and the noisy values (torch_noisy_sample_ys)\n",
    "def loss_function(theta):\n",
    "    return torch.mean((torch_noisy_sample_ys - f_theta(torch_xs, theta)) ** 2)\n",
    "\n",
    "# Initialize the theta parameters with initial values (1.0, 1.0)\n",
    "# requires_grad=True enables gradient computations for these parameters\n",
    "initial_theta = torch.tensor((1., 1.), requires_grad=True)\n",
    "\n",
    "# Compute the initial value of the loss function, with the initial parameters\n",
    "initial_loss = loss_function(initial_theta)\n",
    "print(initial_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "executionInfo": {
     "elapsed": 7841,
     "status": "ok",
     "timestamp": 1745584267470,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "25",
    "outputId": "0c500702-95ed-4086-cd4a-bd2475d311f9"
   },
   "outputs": [],
   "source": [
    "all_results = list()\n",
    "n_iter = 1000\n",
    "\n",
    "theta = copy.deepcopy(initial_theta)\n",
    "opt = torch.optim.SGD([theta], lr=0.01, momentum=0.0)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Compute the loss value for the current parameters theta\n",
    "    loss_value = loss_function(theta)\n",
    "    # Compute the gradients of the loss with respect to theta\n",
    "    loss_value.backward()\n",
    "    # Update the parameters theta using the optimizer and the computed gradients\n",
    "    opt.step()\n",
    "    # Reset gradients to zero to avoid accumulation\n",
    "    opt.zero_grad()\n",
    "    # Add the current loss value to the results list\n",
    "    all_results.append(loss_value.data)\n",
    "\n",
    "print(theta.data)\n",
    "plt.plot(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "26"
   },
   "source": [
    "We have values for the parameters now.\n",
    "Let us look at what they look like.\n",
    "\n",
    "Use the f_theta function on the linspace to plot your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 199,
     "status": "ok",
     "timestamp": 1745522464250,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "27",
    "outputId": "b4310d4b-d9af-4fc6-a4d4-f6b952ea2227"
   },
   "outputs": [],
   "source": [
    "# Compute the values predicted by the linear model f_theta for the linearly spaced points (torch_lsp)\n",
    "# .detach() is used to detach the tensor from the computation graph, meaning that subsequent operations\n",
    "# will not be tracked for gradient computation\n",
    "# .numpy() converts the PyTorch tensor to a NumPy array (for the subsequent plotting here)\n",
    "predicted_ys = f_theta(torch_lsp, theta).detach().numpy()\n",
    "\n",
    "# Plot the original base function as a dashed line\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "\n",
    "# Plot the values predicted by the linear model as a solid line\n",
    "plt.plot(lsp, predicted_ys)\n",
    "\n",
    "# Plot the simulated data (noisy samples)\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 1497,
     "status": "ok",
     "timestamp": 1745522472245,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "28",
    "outputId": "3592111e-85da-4454-d8f2-3e5d2ff97ee9"
   },
   "outputs": [],
   "source": [
    "# Initialize the theta parameters with initial values (1.0, 1.0)\n",
    "# requires_grad=True enables gradient computations for these parameters\n",
    "theta_0 = torch.tensor((1., 1.), requires_grad=True)\n",
    "\n",
    "# Set the number of iterations and initialize the optimizer\n",
    "n_iter = 30\n",
    "opt = torch.optim.SGD([theta_0], lr=0.02, momentum=0.0)\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Every 5 iterations, plot the linear model predicted by the linear model\n",
    "    if i % 5 == 0:\n",
    "        predicted_ys = f_theta(torch_lsp, theta_0).detach().numpy()\n",
    "        plt.plot(lsp, predicted_ys, label='Iteration {}'.format(i))\n",
    "\n",
    "    # Compute the loss\n",
    "    loss_value = loss_function(theta_0)\n",
    "    # Compute the gradients\n",
    "    loss_value.backward()\n",
    "    # Update the parameters using the optimizer (and the computed gradients)\n",
    "    opt.step()\n",
    "    # Reset gradients to zero\n",
    "    opt.zero_grad()\n",
    "\n",
    "# Plot the simulated data (noisy samples)\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "29"
   },
   "source": [
    "## Deep Learning with PyTorch\n",
    "\n",
    "We start by training a small MLP using built-in functionalities in `scikit-learn`, with the [`MLPRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 3194,
     "status": "ok",
     "timestamp": 1745587507273,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "30",
    "outputId": "1e84e769-5a35-48bc-e82c-29c7901a413a"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Create an instance of MLPRegressor, a neural network model for regression\n",
    "# max_iter=5000 specifies the maximum number of iterations for training\n",
    "mlp_model = MLPRegressor(max_iter=5000)\n",
    "\n",
    "# Train the MLP model on the data (xs, noisy_sample_ys)\n",
    "# xs are the features and noisy_sample_ys are the target values\n",
    "# .flatten() is used to transform the noisy_sample_ys array into a 1D vector\n",
    "mlp_model.fit(xs, noisy_sample_ys.flatten())\n",
    "\n",
    "# Use the trained model to predict the values corresponding to the linearly spaced points (lsp)\n",
    "predicted_lsp = mlp_model.predict(lsp)\n",
    "\n",
    "# Plot the simulated data (noisy samples)\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "\n",
    "# Plot the predictions of the MLP model\n",
    "plt.plot(lsp, predicted_lsp, color='orange', lw=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "31"
   },
   "source": [
    "MLPRegressor works well for this simple data, but it lacks the more advanced deep learning modeling that PyTorch can offer.\n",
    "Let's start by achieving a similar result to MLPRegressor, but defining our model ourselves and in PyTorch.\n",
    "\n",
    "By default, the MLP Regressor makes the following computational graph :\n",
    "- input gets multiplied by a matrix with 100 parameters, and an additional parameter is added to each values, giving 100 outputs y (shape = (n_samples, 100))\n",
    "- ReLU is applied to each of these outputs (shape = (n_samples, 100)). The relu function is implemented in PyTorch with torch.nn.functional.relu(x)\n",
    "- Then this value is multiplied by a matrix to produce a scalar output (again 100 parameters) (shape = (n_samples, 1)) and shifted by an offset.\n",
    "\n",
    "A quick reminder on matrix multiplication : it is an operation that combines one matrix A of shape (m,n) and a matrix B of shape (n,p) into a matrix C of shape (m,p). In PyTorch (and NumPy), you need to call torch.matmul(A,B) to make this computation.\n",
    "\n",
    "To make the two big multiplications, we will use one torch tensor of 100 parameters for each multiplication, with the appropriate shape.Create random starting tensors of parameters.\n",
    "\n",
    "Then implement the asked computation to produce our output from our input. You should debug the operations by ensuring the shapes are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1745587508967,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "32"
   },
   "outputs": [],
   "source": [
    "# Create the network parameters with initial random values drawn from a normal distribution\n",
    "# These parameters are the weights (w1, w2) and biases (b1, b2) of the neural network\n",
    "# We use torch.normal to generate these random values, with mean 0.0 and std 0.1 to get small initial values\n",
    "# Don't forget the requires_grad=True that enables gradient computations for these parameters during optimization\n",
    "\n",
    "# First set of weights w1, of size (1, 100)\n",
    "# It is applied to a single input feature and maps it to 100 neurons in the hidden layer\n",
    "w1 = torch.normal(mean=0., std=0.1, size=(1, 100), requires_grad=True)\n",
    "\n",
    "# First set of biases b1 is of size (1, 100)\n",
    "# It corresponds to the biases for each neuron in the first layer\n",
    "b1 = torch.normal(mean=0., std=0.1, size=(1, 100), requires_grad=True)\n",
    "\n",
    "# Second set of weights w2, of size (100, 1)\n",
    "# It corresponds to the weights connecting the 100 neurons in the hidden layer to the single output neuron\n",
    "w2 = torch.normal(mean=0., std=0.1, size=(100, 1), requires_grad=True)\n",
    "\n",
    "# Second set of biases b2, of size (1,)\n",
    "# It corresponds to the bias for the output neuron\n",
    "b2 = torch.normal(mean=0., std=0.1, size=(1,), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1745587509547,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "33",
    "outputId": "79752530-13f6-4ce1-934e-347e979b8c1e"
   },
   "outputs": [],
   "source": [
    "# Define the function f that represents the neural network\n",
    "# It takes as input a tensor x and uses the weights and biases defined previously\n",
    "def f(x, weight1=w1, bias1=b1, weight2=w2, bias2=b2):\n",
    "    # Compute the output of the first layer by performing a matrix multiplication\n",
    "    # between the input x and the weights w1, then adding the bias b1\n",
    "    y1 = torch.matmul(x, weight1) + bias1\n",
    "\n",
    "    # Apply the ReLU activation function to the output of the first layer\n",
    "    a1 = torch.nn.functional.relu(y1)\n",
    "\n",
    "    # Compute the final output by performing a matrix multiplication\n",
    "    # between the activated output a1 and the weights w2, then adding the bias b2\n",
    "    out = torch.matmul(a1, weight2) + bias2\n",
    "\n",
    "    return out\n",
    "\n",
    "# Check that during inference on the data, we obtain an output tensor of shape (80, 1)\n",
    "# This corresponds to 80 predictions, one for each sample in torch_xs\n",
    "f(torch_xs).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "34"
   },
   "source": [
    "Now we will mostly use the optimization procedure above to train our network using Pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745587511199,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "35"
   },
   "outputs": [],
   "source": [
    "n_iter = 2000\n",
    "# The optimizer takes as input a list containing all the parameters of the network: w1, b1, w2, b2\n",
    "opt = torch.optim.SGD([w1, b1, w2, b2], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1466,
     "status": "ok",
     "timestamp": 1745587514655,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "36",
    "outputId": "8642725d-0464-476d-c706-ab61e610f377"
   },
   "outputs": [],
   "source": [
    "# Loop over the specified number of iterations to train the network\n",
    "for i in range(n_iter):\n",
    "    # Perform a forward pass to compute the network's predictions for the input data\n",
    "    prediction = f(torch_xs, w1, b1, w2, b2)\n",
    "\n",
    "    # Compute the loss using the mean squared error between the predictions and the noisy target values\n",
    "    loss = torch.mean((prediction - torch_noisy_sample_ys) ** 2)\n",
    "\n",
    "    # Perform a backward pass to compute the gradients of the loss with respect to the parameters\n",
    "    loss.backward()\n",
    "    # Update the network parameters using the SGD optimizer\n",
    "    opt.step()\n",
    "    # Reset gradients to zero to avoid accumulation of gradients from previous iterations\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # Every 100 iterations, print the iteration number and the current loss value\n",
    "    if not i % 100:\n",
    "        print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745523147080,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "37",
    "outputId": "02944ed3-1e26-4201-f4bb-846cf9c37525"
   },
   "outputs": [],
   "source": [
    "# Compute the values predicted by the neural network model for the linearly spaced points (torch_lsp)\n",
    "# .detach() is used to detach the tensor from the computation graph, meaning that subsequent operations\n",
    "# will not be tracked for gradient computation\n",
    "# .numpy() converts the PyTorch tensor to a NumPy array for plotting\n",
    "predicted_ys = f(torch_lsp).detach().numpy()\n",
    "\n",
    "# Plot the original base function as a dashed line\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "\n",
    "# Plot the values predicted by the neural network model\n",
    "plt.plot(lsp, predicted_ys)\n",
    "\n",
    "# Plot the simulated data (noisy samples)\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "38"
   },
   "source": [
    "Congratulations, you have coded yourself a MLP model ! We have used the computation graph framework.\n",
    "\n",
    "\n",
    "Now let us make our code prettier (more Pytorch) and more efficient.\n",
    "First let us refactor the model in the proper way it should be coded, by using the [`torch.nn.Module`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html) class.\n",
    "You should add almost no new code, just reorganize the one above into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1745523169356,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "39",
    "outputId": "2f481f85-9209-4eb5-8608-ff9658e8b8db"
   },
   "outputs": [],
   "source": [
    "from torch.nn import Module, Parameter\n",
    "\n",
    "# Define a class MyOwnMLP which in inherits from the PyTorch Module class \n",
    "class MyOwnMLP(Module):\n",
    "\n",
    "    # Initialize the parameters of the neural network\n",
    "    def __init__(self):\n",
    "        # Call the constructor from the parent class\n",
    "        super(MyOwnMLP, self).__init__()\n",
    "\n",
    "        # Define the weights and bias for the first layer as parameters of the class\n",
    "        # We initialize them with small values from a normal distribution\n",
    "        self.w1 = Parameter(torch.normal(mean=0., std=0.1, size=(1, 100)))\n",
    "        self.b1 = Parameter(torch.normal(mean=0., std=0.1, size=(1, 100)))\n",
    "\n",
    "        # Define the weights and bias for the second layer in the same way\n",
    "        self.w2 = Parameter(torch.normal(mean=0., std=0.1, size=(100, 1)))\n",
    "        self.b2 = Parameter(torch.normal(mean=0., std=0.1, size=(1,)))\n",
    "\n",
    "    # Define the forward method that specifies the forward pass of the network\n",
    "    def forward(self, x):\n",
    "        # Compute the output of the first layer\n",
    "        y1 = torch.matmul(x, self.w1) + self.b1\n",
    "\n",
    "        # Apply the ReLU activation function to the output of the first layer\n",
    "        a1 = torch.nn.functional.relu(y1)\n",
    "\n",
    "        # Compute the final output of the network\n",
    "        out = torch.matmul(a1, self.w2) + self.b2\n",
    "        return out\n",
    "\n",
    "\n",
    "# Instantiate the MyOwnMLP model\n",
    "model = MyOwnMLP()\n",
    "\n",
    "# Perform a forward pass with the input data torch_xs\n",
    "out = model(torch_xs)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "40"
   },
   "source": [
    "Now we are good to also make the data iteration process look like Pytorch code !\n",
    "\n",
    "We need to define a Dataset object. Once we have this, we can use it to create a DataLoader object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "id": "41"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_x, data_y):\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data_x[idx]\n",
    "        y = self.data_y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356,
     "status": "ok",
     "timestamp": 1745523189120,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "42",
    "outputId": "987a23c4-3c37-4a2d-870c-9c2ad9377873"
   },
   "outputs": [],
   "source": [
    "# Create an instance of CustomDataset with the input data torch_xs and the labels torch_noisy_sample_ys\n",
    "dataset = CustomDataset(data_x=torch_xs, data_y=torch_noisy_sample_ys)\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "# batch_size=10 : the DataLoader will provide batches of 10 samples at a time\n",
    "# num_workers=6 : use 6 processes to load the data in parallel, which can speed up the process\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=10, num_workers=6)\n",
    "\n",
    "# Let's record the time to go through all the data batches\n",
    "start = time.time()\n",
    "\n",
    "# Loop over each batch of data provided by the DataLoader\n",
    "for point in dataloader:\n",
    "    # Here We do nothing with the data, we simply move to the next iteration\n",
    "    pass\n",
    "\n",
    "# Final time is:\n",
    "print('Done in pytorch : ', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "43"
   },
   "source": [
    "The last thing missing to make our pipeline truly Pytorch is to use a GPU.\n",
    "\n",
    "In Pytorch it is really easy, you just need to 'move' your tensors to a 'device'.\n",
    "You can test if a gpu is available and create the appropriate device with the following lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "id": "44"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "\n",
    "# Send the data and the model to the selected device (CPU or GPU)\n",
    "torch_xs = torch_xs.to(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {
    "id": "45"
   },
   "source": [
    "Now we finally have all the elements to make an actual Pytorch complete pipeline !\n",
    "\n",
    "Create a model, and try to put it on a device.\n",
    "Create an optimizer with your model's parameters\n",
    "Make your data into a dataloader\n",
    "\n",
    "Then use two nested for loops : one for 100 epochs, and in each epoch loop over the dataloader\n",
    "    Inside the loop, for every batch first put the data on the device\n",
    "    Then use the semantics of above :\n",
    "        - model(batch)\n",
    "        - loss computation and backward\n",
    "        - gradient step and zero_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2423,
     "status": "ok",
     "timestamp": 1745523265183,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "46",
    "outputId": "921721e3-c692-4136-ab84-af6ee0f8f77d"
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "model = MyOwnMLP()\n",
    "model = model.to(device)\n",
    "\n",
    "# Create an Adam optimizer to adjust the model parameters.\n",
    "# Adam is an optimization algorithm that adapts the learning rate for each parameter\n",
    "# It is another popular optimization algorithm often used to train neural networks\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Create an instance of CustomDataset with the input data torch_xs and the labels torch_noisy_sample_ys\n",
    "dataset = CustomDataset(data_x=torch_xs, data_y=torch_noisy_sample_ys)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=10, num_workers=0)\n",
    "\n",
    "loss = 0\n",
    "# Loop over the specified number of epochs for training\n",
    "for epoch in range(n_epochs):\n",
    "    # Loop over each batch of data provided by the DataLoader\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        # Transfer the batch data to the specified device (GPU or CPU)\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "        # Perform a forward pass to compute the model's predictions for the input batch\n",
    "        prediction = model(batch_x)\n",
    "\n",
    "        # Compute the loss using the mean squared error between the predictions and the target values\n",
    "        loss = torch.mean((prediction - batch_y) ** 2)\n",
    "\n",
    "        # Perform a backward pass to compute the gradients of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters using the Adam optimizer\n",
    "        opt.step()\n",
    "\n",
    "        # Reset gradients to zero to avoid accumulation of gradients from previous iterations\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Convert the loss (tensor) to a scalar value\n",
    "        loss = loss.item()\n",
    "\n",
    "    # Every 10 epochs, print the epoch number and the current loss value\n",
    "    if not epoch % 10:\n",
    "        print(epoch, loss)\n",
    "\n",
    "# Transfer the trained model to the CPU for later use\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {
    "id": "47"
   },
   "source": [
    "Finally, we can plot the last model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 250,
     "status": "ok",
     "timestamp": 1745523269360,
     "user": {
      "displayName": "Giann Karlo",
      "userId": "07023519164898662347"
     },
     "user_tz": -120
    },
    "id": "48",
    "outputId": "c81d41c8-f848-4ca1-b8fd-3178edf0dce2"
   },
   "outputs": [],
   "source": [
    "predicted_ys = model(torch_lsp).detach().numpy()\n",
    "\n",
    "# Plot the original base function as a dashed line\n",
    "plt.plot(lsp, true_ys, linestyle='dashed')\n",
    "\n",
    "# Plot the values predicted by the neural network model\n",
    "plt.plot(lsp, predicted_ys)\n",
    "\n",
    "# Plot the noisy samples\n",
    "plt.scatter(xs, noisy_sample_ys)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {
    "id": "49"
   },
   "source": [
    "This is the end of the practical part of training neural networks !\n",
    "\n",
    "Of course, a lot more can be done. On this simple toy data, you can try to illustrate concepts of this class:\n",
    "- What happens if you use only 10 data points and increase the noise level ?\n",
    "- Can you observe an overfitting behavior ?\n",
    "- Can you see the impact of using different optimisers (SGD vs Adam) ?\n",
    "- ...\n",
    "\n",
    "Another interesting extension is to use a more advanced (yet manageable dataset), such as FashionMnist.\n",
    "You can use it through the built-in PyTorch objects: _torchvision.datasets.FashionMNIST_ .\n",
    "You can install torchvision with _pip install torchvision_ .\n",
    "More generally, you can follow this tutorial: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html to access the data and have a first model example and training:\n",
    "- Can you compare MLP architectures with CNNs on this task ?\n",
    "- Do you see an overfit on this dataset ?\n",
    "- Does data augmentation helps training on this dataset ?\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "introml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
